{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# print(torch.version.cuda)\n",
    "torch.cuda.is_available()\n",
    "# print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHAURYAM DUBEY\\AppData\\Local\\Temp\\ipykernel_41584\\1406316218.py:3: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n",
      "  from snntorch import backprop\n"
     ]
    }
   ],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "\n",
    "# Load FER2013 dataset\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Grayscale(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Change brightness and contrast\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),  # Add small shifts\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='./dataset/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root='./dataset/test', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSNN(nn.Module):\n",
    "    def __init__(self,beta=0.9, slope=25):\n",
    "        super(CSNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with 12 filters in first layer, 64 filters in second, etc.\n",
    "        self.conv1 = nn.Conv2d(1, 32,5)  # Input is 1 channel (grayscale)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout1 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64,5)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "        # Adjusted for smaller output size after convolution and pooling\n",
    "        self.fc1 = nn.Linear(9 *9, 128)\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        # self.dropout3 = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 7)  # 7 classes for FER2013 emotions\n",
    "        self.lif4 = snn.Leaky(\n",
    "            beta=beta, spike_grad=surrogate.fast_sigmoid(slope=slope))\n",
    "        \n",
    "        self.timesteps = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        # Forward pass through convolution and pooling layers\n",
    "        cur1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(cur1, 2)\n",
    "        spk1, mem1 = self.lif1(pool1, mem1)\n",
    "\n",
    "        cur2 = self.conv2(spk1)\n",
    "        pool2 = F.max_pool2d(cur2, 2)\n",
    "        spk2, mem2 = self.lif2(pool2, mem2)\n",
    "        \n",
    "        cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        cur4 = self.fc2(spk3)\n",
    "        spk4, mem4 = self.lif4(cur4, mem4)\n",
    "        \n",
    "        return spk4, mem4\n",
    "\n",
    "        # spk_out = None\n",
    "        # for t in range(self.timesteps):\n",
    "        #     cur_input = x[t]\n",
    "\n",
    "        #     # Layer 1: Conv + Pool + LIF\n",
    "        #     cur1 = self.conv1(cur_input)\n",
    "        #     pool1 = F.max_pool2d(cur1, 2)\n",
    "        #     spk1, mem1 = self.lif1(pool1, mem1)\n",
    "\n",
    "        #     # Layer 2: Conv + Pool + LIF\n",
    "        #     cur2 = self.conv2(spk1)\n",
    "        #     pool2 = F.max_pool2d(cur2, 2)\n",
    "        #     spk2, mem2 = self.lif2(pool2, mem2)\n",
    "\n",
    "        #     # Layer 3: Flatten + FC + LIF\n",
    "        #     cur3 = self.fc1(spk2.view(spk2.size(0), -1))\n",
    "        #     spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "        #     # Layer 4: FC + LIF\n",
    "        #     cur4 = self.fc2(spk3)\n",
    "        #     spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "        #     # Output layer\n",
    "        #     out = self.fc3(spk4)\n",
    "        #     spk_out = out if spk_out is None else spk_out + out\n",
    "\n",
    "        # # Average the output over timesteps\n",
    "        # return spk_out / self.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CSNN(beta=0.9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original beta = 0.9 lr = 0.0001 epochs = 100 accuracy = 49.33% loss < 0.9\n",
    "#2nd try - beta = 0.99 lr = 0.0001 epochs = 20 accuracy = 35.64% loss 1.6463\n",
    "# 3nd try - beta = 0.95 lr = 0.0001 epochs = 20 loss 1.6640\n",
    "# 5nd try - beta = 0.9 lr = 0.001 epochs = 20 accuracy = 28.66% loss 1.7209\n",
    "# 6nd try - beta = 0.9 lr = 0.0001 epochs = 20 accuracy = 35.62% loss 1.6436\n",
    "# 7nd try - beta = 0.8 lr = 0.0001 epochs = 20 accuracy = 33.92% loss 1.6602\n",
    "# 8nd try - beta = 0.8 lr = 0.001 epochs = 20 loss 1.7313\n",
    "# 9nd try - beta = 0.7 lr = 0.0001 epochs = 20 loss 1.6574\n",
    "# 10nd try - beta = 0.5 lr = 0.0001 epochs = 20  loss 1.6670\n",
    "# 11nd try - beta = 0.65 lr = 0.0001 epochs = 20 loss 1.6332\n",
    "# 8nd try - beta = 0.7 lr = 0.0001 epochs = 20 accuracy = 33.92% loss 1.6574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_counts = {3: 7215, 4: 4965,\n",
    "                      5: 4830, 2: 4097, 0: 3995, 6: 3171, 1: 436}\n",
    "class_weights = torch.tensor([1 / train_class_counts[i]\n",
    "                             for i in range(7)]).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# loss_fn = SF.mse_count_loss(correct_rate=1.0,incorrect_rate=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  snn.utils.reset(net)\n",
    "  for step in range(data.size(0)):\n",
    "      spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.1589\n",
      "Epoch [2/100], Loss: 4.1589\n",
      "Epoch [3/100], Loss: 4.1589\n",
      "Epoch [4/100], Loss: 4.1589\n",
      "Epoch [5/100], Loss: 4.1589\n",
      "Epoch [6/100], Loss: 4.1589\n",
      "Epoch [7/100], Loss: 4.1589\n",
      "Epoch [8/100], Loss: 4.1589\n",
      "Epoch [9/100], Loss: 4.1589\n",
      "Epoch [10/100], Loss: 4.1589\n",
      "Epoch [11/100], Loss: 4.1589\n",
      "Epoch [12/100], Loss: 4.1589\n",
      "Epoch [13/100], Loss: 4.1589\n",
      "Epoch [14/100], Loss: 4.1589\n",
      "Epoch [15/100], Loss: 4.1589\n",
      "Epoch [16/100], Loss: 4.1589\n",
      "Epoch [17/100], Loss: 4.1589\n",
      "Epoch [18/100], Loss: 4.1589\n",
      "Epoch [19/100], Loss: 4.1589\n",
      "Epoch [20/100], Loss: 4.1589\n",
      "Epoch [21/100], Loss: 4.1589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 60\u001b[0m\n\u001b[0;32m     53\u001b[0m             running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrain_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 33\u001b[0m, in \u001b[0;36mtrain_snn\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# images = spikegen.rate(images,num_steps=10 )\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# spk_rec,_= net(images)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# spk_rec,_,_= net(images)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m spk_rec\u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# print(spk_rec.size(),epoch)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# spk_rec.squeeze(1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# print(spk_rec.size())\u001b[39;00m\n\u001b[0;32m     42\u001b[0m labels_onehot \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(labels, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n",
      "Cell \u001b[1;32mIn[83], line 5\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(net, data)\u001b[0m\n\u001b[0;32m      3\u001b[0m snn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mreset(net)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m----> 5\u001b[0m     spk_out, mem_out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     spk_rec\u001b[38;5;241m.\u001b[39mappend(spk_out)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(spk_rec)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[77], line 41\u001b[0m, in \u001b[0;36mCSNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m cur2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(spk1)\n\u001b[0;32m     40\u001b[0m pool2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(cur2, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m spk2, mem2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlif2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m cur3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(spk2\u001b[38;5;241m.\u001b[39mview(spk2\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     44\u001b[0m spk3, mem3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif3(cur3, mem3)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\snntorch\\_neurons\\leaky.py:210\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[1;34m(self, input_, mem)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\snntorch\\_neurons\\leaky.py:243\u001b[0m, in \u001b[0;36mLeaky._base_sub\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_sub\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_):\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_state_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from snntorch import spikegen\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "# Training loop\n",
    "\n",
    "\n",
    "\n",
    "def train_snn(num_epochs):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # images = add_noise(images)\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long()\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # images = spikegen.rate(images,num_steps=10 )\n",
    "            # spk_rec,_= net(images)\n",
    "            # spk_rec,_,_= net(images)\n",
    "\n",
    "            spk_rec= forward_pass(net, images)\n",
    "\n",
    "            # print(spk_rec.size(),epoch)\n",
    "\n",
    "            # spk_rec.squeeze(1)\n",
    "\n",
    "            # labels = labels.view(-1)\n",
    "\n",
    "            # print(spk_rec.size())\n",
    "            labels_onehot = F.one_hot(labels, num_classes=7).long()\n",
    "            loss = loss_fn(spk_rec, labels_onehot)\n",
    "\n",
    "\n",
    "            # Backward pass and optimization\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "train_snn(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 24.75%\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = spikegen.rate(images, num_steps=10)\n",
    "            outputs, a, _ = net(images)\n",
    "            # outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'csnn2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, data):\n",
    "  spk_rec = []\n",
    "  snn.utils.reset(net)\n",
    "  for step in range(data.size(0)):\n",
    "      out, spk_out, mem_out = net(data[step])\n",
    "      spk_rec.append(spk_out)\n",
    "  return torch.stack(spk_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 7])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Gradient calculation + weight update\u001b[39;00m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Store loss history for future plotting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "counter = 0\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "test_acc_hist = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(train_loader)):\n",
    "        # Downsampling image from (128 x 128) to (32 x 32)\n",
    "        # data = nn.functional.interpolate(data, size=(48, 48))\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        # propagating one batch through the network and evaluating loss\n",
    "        # spk_rec = forward_pass(net, data)\n",
    "        data = spikegen.rate(data,num_steps=10)\n",
    "        spk_rec = net(data)\n",
    "        # targets_one_hot = F.one_hot(targets, num_classes=7).long()\n",
    "        loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        acc_hist.append(acc)\n",
    "\n",
    "        # print metrics every so often\n",
    "        if counter % 16 == 0:\n",
    "          print(\n",
    "              f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "          print(f\"Train Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "          correct = 0\n",
    "          total = 0\n",
    "\n",
    "          for i, (data, targets) in enumerate(iter(test_loader)):\n",
    "            # data = nn.functional.interpolate(data, size=(48,48))\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            data = spikegen.rate(data,num_steps=10)\n",
    "            spk_rec = net(data)\n",
    "            # spk_rec = forward_pass(net, data)\n",
    "            correct += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "\n",
    "          test_acc = (correct/total) * 100\n",
    "          test_acc_hist.append(test_acc)\n",
    "          print(f\"========== Test Set Accuracy: {test_acc:.2f}% ==========\\n\")\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_snn(num_epochs, train_loader, val_loader, timesteps):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        net.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Rate encoding for training\n",
    "            spike_trains = spikegen.rate(images, timesteps)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(spike_trains)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}\")\n",
    "        # acc = SF.accuracy_rate(spk_rec, targets)\n",
    "        # acc_hist.append(acc)\n",
    "\n",
    "        # Validation Phase\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Rate encoding for validation\n",
    "                spike_trains = spikegen.rate(images, timesteps)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = net(spike_trains)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 7])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.6027\n",
      "Epoch [1/20], Validation Loss: 0.5997, Accuracy: 24.75%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_snn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[102], line 15\u001b[0m, in \u001b[0;36mtrain_snn\u001b[1;34m(num_epochs, train_loader, val_loader, timesteps)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike_trains\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[97], line 64\u001b[0m, in \u001b[0;36mCSNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m cur1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(cur_input)\n\u001b[0;32m     63\u001b[0m pool1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(cur1, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m spk1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlif1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Layer 2: Conv + Pool + LIF\u001b[39;00m\n\u001b[0;32m     67\u001b[0m cur2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(spk1)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\snntorch\\_neurons\\leaky.py:209\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[1;34m(self, input_, mem)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m input_\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_function(input_)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n",
      "File \u001b[1;32mc:\\Users\\SHAURYAM DUBEY\\Documents\\.repo-github\\csnn\\venv\\lib\\site-packages\\snntorch\\_neurons\\neurons.py:106\u001b[0m, in \u001b[0;36mSpikingNeuron.mem_reset\u001b[1;34m(self, mem)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates detached reset signal if mem > threshold.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mReturns reset.\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m mem_shift \u001b[38;5;241m=\u001b[39m mem \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n\u001b[1;32m--> 106\u001b[0m reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmem_shift\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reset\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_snn(num_epochs, train_loader, test_loader, timesteps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
